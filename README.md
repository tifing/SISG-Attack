# SISG-Attack  
**Jailbreaking Multimodal LLMs via Structure-Induced Safety Gaps**


SISG-Attack is a layout-guided jailbreak method that transforms harmful inputs into structured multimodal layouts—by converting text into tables and embedding images within cells—to elongate reasoning chains and delay or suppress safety mechanisms in multimodal language models.


**Included**  
This repository provides the core implementation used to run SISG-Attack against GPT-4o on the HADES benchmark, together with layout generators and a pluggable judge module for automated evaluation; the codebase is modular so adapting the scripts to other datasets or MLLMs requires only minimal configuration changes.


**Results**  
Evaluation artifacts for GPT-4o runs are available here:  


**Disclaimer**  
For academic research on model robustness and safety only. Use responsibly and comply with the terms of any model providers.


